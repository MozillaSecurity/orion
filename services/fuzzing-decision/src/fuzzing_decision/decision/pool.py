# This Source Code Form is subject to the terms of the Mozilla Public License,
# v. 2.0. If a copy of the MPL was not distributed with this file, You can
# obtain one at http://mozilla.org/MPL/2.0/.

from __future__ import annotations

import logging
import math
import os
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from itertools import chain
from pathlib import Path
from string import Template
from typing import Any, Iterator

import dateutil.parser
import yaml
from taskcluster.exceptions import TaskclusterFailure, TaskclusterRestFailure
from taskcluster.utils import fromNow, slugId, stringDate
from tcadmin.resources import Hook, Role
from tcadmin.resources import WorkerPool as TCWorkerPool

from ..common import taskcluster
from ..common.pool import (
    CPU_ALIASES,
    ConfigurationError,
    FuzzingPoolConfig,
    MachineTypes,
)
from ..common.util import parse_size, parse_time, validate_schema_by_name
from . import (
    CANCEL_TASK_DAYS,
    DECISION_TASK_SECRET,
    HOOK_PREFIX,
    OWNER_EMAIL,
    PROVIDER_IDS,
    PROVISIONER_ID,
    SCHEDULER_ID,
    WORKER_POOL_PREFIX,
)
from .providers import Provider

LOG = logging.getLogger(__name__)

DESCRIPTION = """*DO NOT EDIT* - This resource is configured automatically.

Fuzzing resources generated by https://github.com/MozillaSecurity/orion/tree/master/services/fuzzing-decision"""

DOCKER_WORKER_DEVICES = (
    "cpu",
    "hostSharedMemory",
    "loopbackAudio",
    "loopbackVideo",
    "kvm",
)

TEMPLATES = (Path(__file__).parent / "task_templates").resolve()
DECISION_TASK = Template((TEMPLATES / "decision.yaml").read_text())
FUZZING_TASK = Template((TEMPLATES / "fuzzing.yaml").read_text())


class MountArtifactResolver:
    CACHE: dict[str, int] = {}  # noqa: RUF012 cache of orion service -> taskId

    @classmethod
    def lookup_taskid(cls, namespace: str) -> int:
        if namespace not in cls.CACHE:
            # need to resolve "image" to a task ID where the mount
            # artifact is
            idx = taskcluster.get_service("index")
            result = idx.findTask(namespace)
            cls.CACHE[namespace] = result["taskId"]

        return cls.CACHE[namespace]


def add_task_image(task: dict[str, Any], config: FuzzingPoolConfig) -> None:
    """Add image or mount to task payload, depending on platform."""
    # generic-worker linux still uses docker images, but need to be loaded
    # into podman, can't use mounts to do it
    # TODO: should still have task dependency for indexed-image
    if config.worker == "generic" and config.platform != "linux":
        assert isinstance(config.container, dict)
        assert config.container["type"] != "docker-image"
        task_id: str | int
        if config.container["type"] == "indexed-image":
            # need to resolve "image" to a task ID where the mount artifact is
            task_id = MountArtifactResolver.lookup_taskid(config.container["namespace"])
        else:
            task_id = config.container["taskId"]
        task["payload"].setdefault("mounts", [])
        suffixes = Path(config.container["path"]).suffixes
        if len(suffixes) >= 2 and suffixes[-2] == ".tar":
            fmt = f"tar{suffixes[-1]}"
        else:
            assert suffixes, "unable to determine format from container path"
            fmt = suffixes[-1][1:]
        task["payload"]["mounts"].append(
            {
                "format": fmt,
                "content": {
                    "taskId": task_id,
                    "artifact": config.container["path"],
                },
                "directory": ".",
            }
        )
        task["dependencies"].append(task_id)
    elif config.worker in {"docker", "d2g"}:
        # `container` can be either a string or a dict, so can't template it
        task["payload"]["image"] = config.container
        if (
            isinstance(task["payload"]["image"], dict)
            and task["payload"]["image"].get("type") == "task-image"
        ):
            task["dependencies"].append(task["payload"]["image"]["taskId"])
    else:
        raise NotImplementedError(f"{config.worker} / {config.platform} not supported")


def add_capabilities_for_scopes(task: dict[str, Any]) -> None:
    """Request capabilities to match the scopes specified by the task"""
    capabilities = task["payload"].setdefault("capabilities", {})
    scopes = set(task["scopes"])
    capabilities.setdefault("devices", {})
    for device in DOCKER_WORKER_DEVICES:
        if f"docker-worker:capability:device:{device}" in scopes:
            capabilities["devices"][device] = True
    if "docker-worker:capability:privileged" in scopes:
        capabilities["privileged"] = True
    if not capabilities["devices"]:
        del capabilities["devices"]
    if not capabilities:
        del task["payload"]["capabilities"]


def configure_task(
    task: dict[str, Any],
    config: FuzzingPoolConfig,
    now: datetime,
    env: dict[str, str] | None,
) -> None:
    task["payload"]["artifacts"].update(
        artifact_map(config, stringDate(fromNow("4 weeks", now)))
    )
    task["routes"] = sorted(set(config.routes + task["routes"]))
    task["scopes"] = sorted(set(chain(get_scopes(config), task["scopes"])))
    add_capabilities_for_scopes(task)
    add_task_image(task, config)
    if config.platform == "windows":
        task["payload"]["env"]["MSYSTEM"] = "MINGW64"
        task["payload"]["command"] = [
            "set HOME=%CD%",
            "set ARTIFACTS=%CD%",
            "set PATH="
            + ";".join(
                [
                    r"%CD%\msys64\opt\python",
                    r"%CD%\msys64\opt\python\Scripts",
                    r"%CD%\msys64\MINGW64\bin",
                    r"%CD%\msys64\usr\bin",
                    "%PATH%",
                ]
            ),
            "fuzzing-pool-launch",
        ]
        if config.run_as_admin:
            task["payload"].setdefault("osGroups", [])
            task["payload"]["osGroups"].append("Administrators")
            task["payload"]["features"]["runAsAdministrator"] = True
        task["payload"].setdefault("onExitStatus", {})
        task["payload"]["onExitStatus"].setdefault("retry", [])
        # system restart
        task["payload"]["onExitStatus"]["retry"].append(0x40010004)
    elif config.platform == "macos":
        task["payload"]["command"] = [
            [
                "/bin/bash",
                "-c",
                "-x",
                'eval "$(homebrew/bin/brew shellenv)" && exec fuzzing-pool-launch',
            ],
        ]

    if config.platform in {"macos", "windows"}:
        # translate artifacts from dict to array for generic-worker
        task["payload"]["artifacts"] = [
            # `... or artifact` because dict.update returns None
            artifact.update({"name": name}) or artifact
            for name, artifact in task["payload"]["artifacts"].items()
        ]
    if env is not None:
        if not set(task["payload"]["env"]).isdisjoint(set(env)):
            overwrites = set(task["payload"]["env"]) & set(env)
            raise ConfigurationError(
                f"Config {config.pool_id} overwrites required env vars: "
                f"{','.join(overwrites)}"
            )
        task["payload"]["env"].update(env)


def cancel_tasks(worker_type: str) -> None:
    # Avoid cancelling self
    self_task_id = os.getenv("TASK_ID")

    hooks = taskcluster.get_service("hooks")
    queue = taskcluster.get_service("queue")

    # cycle only hook fires after this limit
    cycle_limit = datetime.now(timezone.utc) - timedelta(days=CANCEL_TASK_DAYS)

    # Get tasks by hook
    def iter_tasks_by_hook(hook_id: str):
        try:
            for fire in hooks.listLastFires(HOOK_PREFIX, hook_id)["lastFires"]:
                if fire["result"] != "success":
                    continue
                task_created_time = dateutil.parser.isoparse(fire["taskCreateTime"])
                if task_created_time < cycle_limit:
                    continue
                try:
                    result = queue.listTaskGroup(fire["taskId"])
                except TaskclusterFailure as exc:
                    if "No task-group with taskGroupId" in str(exc):
                        continue
                    raise
                while result.get("continuationToken"):
                    yield from [
                        (task, (fire["firedBy"] == "schedule"))
                        for task in result["tasks"]
                    ]
                    result = queue.listTaskGroup(
                        fire["taskId"],
                        query={"continuationToken": result["continuationToken"]},
                    )
                yield from [
                    (task, (fire["firedBy"] == "schedule")) for task in result["tasks"]
                ]
        except TaskclusterRestFailure as msg:
            if "No such hook" in str(msg):
                return None
            raise

    tasks_to_cancel = []
    for task, scheduled in iter_tasks_by_hook(worker_type):
        task_id = task["status"]["taskId"]

        if task_id == self_task_id:
            if scheduled:
                # if this decision task was the result of a scheduled hook, don't
                # cancel anything. if cycle_time is shorter than max_run_time, we
                # want prior tasks to remain running
                LOG.info(f"{self_task_id} is scheduled, not cancelling tasks")
                return None
            # avoid cancelling self
            continue

        # State can be pending,running,completed,failed,exception
        # We only cancel pending & running tasks
        if any(
            run["state"] in {"pending", "running"} for run in task["status"]["runs"]
        ):
            tasks_to_cancel.append(task_id)
    LOG.info(f"{self_task_id} is cancelling {len(tasks_to_cancel)} tasks")

    for task_id in tasks_to_cancel:
        # Cancel the task
        try:
            LOG.warning(f"=> cancelling: {task_id}")
            queue.cancelTask(task_id)
        except Exception:
            LOG.exception(f"Exception calling cancelTask({task_id})")


def get_scopes(pool: FuzzingPoolConfig) -> list[str]:
    result = pool.scopes.copy()

    if pool.platform == "windows" and pool.run_as_admin:
        result.extend(
            (
                (
                    "generic-worker:"
                    f"os-group:{PROVISIONER_ID}/{pool.hook_id}/Administrators"
                ),
                (
                    "generic-worker:"
                    f"run-as-administrator:{PROVISIONER_ID}/{pool.hook_id}"
                ),
            )
        )

    result.extend(f"queue:route:{route}" for route in pool.routes)

    return result


def build_resources(
    pools: list[FuzzingPoolConfig],
    providers: dict[str, Provider],
    machine_type_db: MachineTypes,
    env: dict[str, str] | None = None,
) -> Iterator[TCWorkerPool | Hook | Role]:
    """Build the full tc-admin resources to compare and build the pool"""

    assert pools
    pool = pools[0]

    if len(pools) > 1:
        # apply_to pool
        max_capacity = max(sum(p.tasks for p in pools if p.tasks) * 2, 3)
    else:
        # * 2 since Taskcluster seems to not reuse workers very quickly in some
        # cases, so we end up with a lot of pending tasks.
        max_capacity = (
            max(1, math.ceil(pool.max_run_time / pool.cycle_time)) * pool.tasks * 2
        )

    # Build the pool configuration for selected machines
    worker = WorkerPool(
        cloud=pool.cloud,
        cpu=pool.cpu,
        demand=pool.demand,
        disk_size=pool.disk_size,
        imageset=pool.imageset,
        include_tasks_in_taskmanager=True,
        machine_types=pool.machine_types,
        max_capacity=max_capacity,
        min_capacity=0,
        name=pool.hook_id,
        nested_virtualization=pool.nested_virtualization,
        performance_monitoring_unit=pool.performance_monitoring_unit,
        owner=OWNER_EMAIL,
        platform=pool.platform,
        worker=pool.worker,
    )
    yield from worker.build_resources(providers, machine_type_db)

    # Build the decision task payload that will trigger the new fuzzing tasks
    decision_task = yaml.safe_load(
        DECISION_TASK.substitute(
            description=DESCRIPTION.replace("\n", "\\n"),
            max_run_time=parse_time("1h"),
            owner_email=OWNER_EMAIL,
            pool_id=pool.config_pool_id,
            provisioner=PROVISIONER_ID,
            scheduler=SCHEDULER_ID,
            secret=DECISION_TASK_SECRET,
            task_id=pool.hook_id,
        )
    )
    if env is not None:
        assert set(decision_task["payload"]["env"]).isdisjoint(set(env))
        decision_task["payload"]["env"].update(env)

    cycle_crons = list(pool.cycle_crons())

    scopes = [
        *sorted(
            set(
                chain(
                    decision_task["scopes"],
                    chain.from_iterable(get_scopes(p) for p in pools),
                )
            )
        ),
        "queue:create-task:highest:proj-fuzzing/ci-decision",
    ]
    decision_task["scopes"] = [f"assume:hook-id:{HOOK_PREFIX}/{pool.hook_id}"]

    yield Hook(
        bindings=(),
        description=DESCRIPTION,
        emailOnError=True,
        hookGroupId=HOOK_PREFIX,
        hookId=pool.hook_id,
        name=pool.hook_id,
        owner=OWNER_EMAIL,
        schedule=cycle_crons,
        task=decision_task,
        triggerSchema={},
    )

    yield Role(
        description=DESCRIPTION,
        roleId=f"hook-id:{HOOK_PREFIX}/{pool.hook_id}",
        scopes=scopes,
    )


def artifact_map(pool: FuzzingPoolConfig, expires: str) -> dict[str, dict[str, str]]:
    result = {}
    for local_path, value in pool.artifacts.items():
        assert isinstance(value["url"], str)
        assert value["type"] in {"directory", "file"}
        result[value["url"]] = {
            "expires": expires,
            "path": local_path,
            "type": value["type"],
        }
    # this artifact is required by pool_launch
    result["project/fuzzing/private/logs"] = {
        "expires": expires,
        "path": "/logs/" if pool.platform == "linux" else "logs",
        "type": "directory",
    }
    return result


def build_tasks(
    pool: FuzzingPoolConfig, parent_task_id: str, env: dict[str, str] | None = None
) -> Iterator[tuple[str, dict]]:
    """Create fuzzing tasks and attach them to a decision task"""
    now = datetime.now(timezone.utc)
    preprocess_task_id = None

    for preprocess in pool.get_preprocess():
        task = yaml.safe_load(
            FUZZING_TASK.substitute(
                created=stringDate(now),
                deadline=stringDate(
                    now
                    + min(timedelta(days=5), timedelta(seconds=preprocess.cycle_time))
                ),
                description=DESCRIPTION.replace("\n", "\\n"),
                expires=stringDate(fromNow("4 weeks", now)),
                max_run_time=preprocess.max_run_time,
                name=f"Fuzzing task {pool.task_id} - preprocess",
                owner_email=OWNER_EMAIL,
                pool_id=pool.pool_id,
                provisioner=PROVISIONER_ID,
                scheduler=SCHEDULER_ID,
                secret=DECISION_TASK_SECRET,
                task_group=parent_task_id,
                task_id=pool.hook_id,
            )
        )
        task["payload"]["env"]["TASKCLUSTER_FUZZING_PREPROCESS"] = "1"
        configure_task(task, preprocess, now, env)
        preprocess_task_id = slugId()
        yield preprocess_task_id, task

    for i in range(1, pool.tasks + 1):
        task = yaml.safe_load(
            FUZZING_TASK.substitute(
                created=stringDate(now),
                deadline=stringDate(
                    now + min(timedelta(days=5), timedelta(seconds=pool.cycle_time))
                ),
                description=DESCRIPTION.replace("\n", "\\n"),
                expires=stringDate(fromNow("4 weeks", now)),
                max_run_time=pool.max_run_time,
                name=f"Fuzzing task {pool.task_id} - {i}/{pool.tasks}",
                owner_email=OWNER_EMAIL,
                pool_id=pool.pool_id,
                provisioner=PROVISIONER_ID,
                scheduler=SCHEDULER_ID,
                secret=DECISION_TASK_SECRET,
                task_group=parent_task_id,
                task_id=pool.hook_id,
            )
        )
        if preprocess_task_id is not None:
            task["dependencies"].append(preprocess_task_id)
        configure_task(task, pool, now, env)
        yield slugId(), task


@dataclass
class WorkerPool:
    cloud: str
    cpu: str
    demand: bool
    disk_size: int
    imageset: str
    include_tasks_in_taskmanager: bool
    machine_types: list[str]
    max_capacity: int
    min_capacity: int
    name: str
    nested_virtualization: bool
    performance_monitoring_unit: bool
    owner: str
    platform: str
    worker: str

    @classmethod
    def from_file_iter(cls, pools_yml: Path) -> Iterator[WorkerPool]:
        # these should match what's in workers.yaml!
        defaults = {
            "cloud": "gcp",
            "cpu": "x86_64",
            "demand": False,
            "disk_size": "60g",
            "include_tasks_in_taskmanager": False,
            "min_capacity": 0,
            "nested_virtualization": False,
            "performance_monitoring_unit": False,
            "platform": "linux",
            "worker": "d2g",
        }
        with pools_yml.open() as pools_fd:
            data = yaml.safe_load(pools_fd)
        # apply defaults
        for idx, config in enumerate(data):
            this_config = defaults.copy()
            this_config.update(config)
            data[idx] = this_config
        validate_schema_by_name(instance=data, name="WorkerPools")
        for pool_config in data:
            pool_config["disk_size"] = int(
                parse_size(pool_config["disk_size"]) / parse_size("1g")
            )
            pool_config["cpu"] = CPU_ALIASES.get(pool_config["cpu"], pool_config["cpu"])
            yield cls(**pool_config)

    def build_resources(
        self,
        providers: dict[str, Provider],
        machine_type_db: MachineTypes,
    ) -> Iterator[TCWorkerPool]:
        """Build the full tc-admin resources to compare and build the pool"""
        # Select a cloud provider according to configuration
        provider = providers[self.cloud]

        # Build the pool configuration for selected machines
        def _get_machine_list():
            for machine in self.machine_types:
                zone_blacklist = machine_type_db.zone_blacklist(
                    self.cloud, self.cpu, machine
                )
                yield (machine, zone_blacklist)

        config: dict[str, object] = {
            "launchConfigs": provider.build_launch_configs(
                self.imageset,
                _get_machine_list(),
                self.disk_size,
                self.platform,
                self.demand,
                self.nested_virtualization,
                self.performance_monitoring_unit,
                self.worker,
            ),
            "maxCapacity": self.max_capacity,
            "minCapacity": self.min_capacity,
        }
        config["lifecycle"] = {
            # give workers 15 minutes to register before assuming they're broken
            "registrationTimeout": parse_time("15m"),
            "reregistrationTimeout": parse_time("4d"),
            "queueInactivityTimeout": parse_time("2h"),
        }

        if self.cloud != "static":
            yield TCWorkerPool(
                config=config,
                description=DESCRIPTION,
                emailOnError=True,
                owner=self.owner,
                providerId=PROVIDER_IDS[self.cloud],
                workerPoolId=f"{WORKER_POOL_PREFIX}/{self.name}",
            )
