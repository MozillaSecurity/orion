# -*- coding: utf-8 -*-

# This Source Code Form is subject to the terms of the Mozilla Public License,
# v. 2.0. If a copy of the MPL was not distributed with this file, You can
# obtain one at http://mozilla.org/MPL/2.0/.

import logging
import math
import os
from datetime import datetime, timedelta
from itertools import chain
from pathlib import Path
from string import Template

import yaml
from taskcluster.exceptions import TaskclusterFailure, TaskclusterRestFailure
from taskcluster.utils import fromNow, slugId, stringDate
from tcadmin.resources import Hook, Role, WorkerPool

from ..common import taskcluster
from ..common.pool import PoolConfigMap as CommonPoolConfigMap
from ..common.pool import PoolConfiguration as CommonPoolConfiguration
from ..common.pool import parse_time
from . import (
    DECISION_TASK_SECRET,
    HOOK_PREFIX,
    OWNER_EMAIL,
    PROVIDER_IDS,
    PROVISIONER_ID,
    SCHEDULER_ID,
    WORKER_POOL_PREFIX,
)

LOG = logging.getLogger(__name__)

DESCRIPTION = """*DO NOT EDIT* - This resource is configured automatically.

Fuzzing resources generated by https://github.com/MozillaSecurity/orion/tree/master/services/fuzzing-decision"""  # noqa

DOCKER_WORKER_DEVICES = (
    "cpu",
    "hostSharedMemory",
    "loopbackAudio",
    "loopbackVideo",
    "kvm",
)

TEMPLATES = (Path(__file__).parent / "task_templates").resolve()
DECISION_TASK = Template((TEMPLATES / "decision.yaml").read_text())
FUZZING_TASK = Template((TEMPLATES / "fuzzing.yaml").read_text())


def add_capabilities_for_scopes(task):
    """Request capabilities to match the scopes specified by the task"""
    capabilities = task["payload"].setdefault("capabilities", {})
    scopes = set(task["scopes"])
    capabilities.setdefault("devices", {})
    for device in DOCKER_WORKER_DEVICES:
        if f"docker-worker:capability:device:{device}" in scopes:
            capabilities["devices"][device] = True
    if "docker-worker:capability:privileged" in scopes:
        capabilities["privileged"] = True
    if not capabilities["devices"]:
        del capabilities["devices"]


def cancel_tasks(worker_type):
    # Avoid cancelling self
    self_task_id = os.getenv("TASK_ID")

    hooks = taskcluster.get_service("hooks")
    queue = taskcluster.get_service("queue")

    # Get tasks by hook
    def iter_tasks_by_hook(hook_id):
        try:
            for fire in hooks.listLastFires(HOOK_PREFIX, hook_id)["lastFires"]:
                if fire["result"] != "success":
                    continue
                try:
                    result = queue.listTaskGroup(fire["taskId"])
                except TaskclusterFailure as exc:
                    if "No task-group with taskGroupId" in str(exc):
                        continue
                    raise
                while result.get("continuationToken"):
                    yield from [
                        (task, (fire["firedBy"] == "schedule"))
                        for task in result["tasks"]
                    ]
                    result = queue.listTaskGroup(
                        fire["taskId"],
                        query={"continuationToken": result["continuationToken"]},
                    )
                yield from [
                    (task, (fire["firedBy"] == "schedule")) for task in result["tasks"]
                ]
        except TaskclusterRestFailure as msg:
            if "No such hook" in str(msg):
                return
            raise

    tasks_to_cancel = []
    for task, scheduled in iter_tasks_by_hook(worker_type):
        task_id = task["status"]["taskId"]

        if task_id == self_task_id:
            if scheduled:
                # if this decision task was the result of a scheduled hook, don't
                # cancel anything. if cycle_time is shorter than max_run_time, we
                # want prior tasks to remain running
                LOG.info(f"{self_task_id} is scheduled, not cancelling tasks")
                return
            # avoid cancelling self
            continue

        # State can be pending,running,completed,failed,exception
        # We only cancel pending & running tasks
        if any(
            run["state"] in {"pending", "running"} for run in task["status"]["runs"]
        ):
            tasks_to_cancel.append(task_id)
    LOG.info(f"{self_task_id} is cancelling {len(tasks_to_cancel)} tasks")

    for task_id in tasks_to_cancel:
        # Cancel the task
        try:
            LOG.warning(f"=> cancelling: {task_id}")
            queue.cancelTask(task_id)
        except Exception:
            LOG.exception(f"Exception calling cancelTask({task_id})")


class PoolConfiguration(CommonPoolConfiguration):
    @property
    def task_id(self):
        return f"{self.platform}-{self.pool_id}"

    def build_resources(self, providers, machine_types, env=None):
        """Build the full tc-admin resources to compare and build the pool"""

        # Select a cloud provider according to configuration
        assert self.cloud in providers, f"Cloud Provider {self.cloud} not available"
        provider = providers[self.cloud]

        # Build the pool configuration for selected machines
        machines = self.get_machine_list(machine_types)
        config = {
            "minCapacity": 0,
            "maxCapacity": (
                # add +1 to expected size, so if we manually trigger the hook, the new
                # decision can run without also manually cancelling a task
                # * 2 since Taskcluster seems to not reuse workers very quickly in some
                # cases, so we end up with a lot of pending tasks.
                max(1, math.ceil(self.max_run_time / self.cycle_time)) * self.tasks * 2
                + 1
            ),
            "launchConfigs": provider.build_launch_configs(
                self.imageset, machines, self.disk_size
            ),
            "lifecycle": {
                # give workers 15 minutes to register before assuming they're broken
                "registrationTimeout": parse_time("15m"),
                "reregistrationTimeout": parse_time("4d"),
            },
        }

        # Build the decision task payload that will trigger the new fuzzing tasks
        decision_task = yaml.safe_load(
            DECISION_TASK.substitute(
                description=DESCRIPTION.replace("\n", "\\n"),
                pool_id=self.pool_id,
                owner_email=OWNER_EMAIL,
                task_id=self.task_id,
                provisioner=PROVISIONER_ID,
                scheduler=SCHEDULER_ID,
                max_run_time=parse_time("1h"),
                secret=DECISION_TASK_SECRET,
            )
        )
        decision_task["scopes"] = sorted(chain(decision_task["scopes"], self.scopes))
        add_capabilities_for_scopes(decision_task)
        if env is not None:
            assert set(decision_task["payload"]["env"]).isdisjoint(set(env))
            decision_task["payload"]["env"].update(env)

        pool = WorkerPool(
            workerPoolId=f"{WORKER_POOL_PREFIX}/{self.task_id}",
            providerId=PROVIDER_IDS[self.cloud],
            description=DESCRIPTION,
            owner=OWNER_EMAIL,
            emailOnError=True,
            config=config,
        )

        hook = Hook(
            hookGroupId=HOOK_PREFIX,
            hookId=self.task_id,
            name=self.task_id,
            description=DESCRIPTION,
            owner=OWNER_EMAIL,
            emailOnError=True,
            schedule=list(self.cycle_crons()),
            task=decision_task,
            bindings=(),
            triggerSchema={},
        )

        role = Role(
            roleId=f"hook-id:{HOOK_PREFIX}/{self.task_id}",
            description=DESCRIPTION,
            scopes=decision_task["scopes"],
        )

        return [pool, hook, role]

    def artifact_map(self, expires):
        result = {}
        for local_path, value in self.artifacts.items():
            assert isinstance(value["url"], str)
            assert value["type"] in {"directory", "file"}
            result[value["url"]] = {
                "expires": expires,
                "path": local_path,
                "type": value["type"],
            }
        # this artifact is required by pool_launch
        result["project/fuzzing/private/logs"] = {
            "expires": expires,
            "path": "/logs/",
            "type": "directory",
        }
        return result

    def build_tasks(self, parent_task_id, env=None):
        """Create fuzzing tasks and attach them to a decision task"""
        now = datetime.utcnow()
        preprocess_task_id = None

        preprocess = self.create_preprocess()
        if preprocess is not None:
            task = yaml.safe_load(
                FUZZING_TASK.substitute(
                    task_group=parent_task_id,
                    created=stringDate(now),
                    deadline=stringDate(
                        now + timedelta(seconds=preprocess.max_run_time)
                    ),
                    expires=stringDate(fromNow("1 week", now)),
                    task_id=self.task_id,
                    pool_id=self.pool_id,
                    provisioner=PROVISIONER_ID,
                    description=DESCRIPTION.replace("\n", "\\n"),
                    owner_email=OWNER_EMAIL,
                    scheduler=SCHEDULER_ID,
                    secret=DECISION_TASK_SECRET,
                    max_run_time=preprocess.max_run_time,
                    name=f"Fuzzing task {self.task_id} - preprocess",
                )
            )
            task["payload"]["artifacts"].update(
                preprocess.artifact_map(stringDate(fromNow("1 week", now)))
            )
            # `container` can be either a string or a dict, so can't template it
            task["payload"]["image"] = preprocess.container
            task["payload"]["env"]["TASKCLUSTER_FUZZING_PREPROCESS"] = "1"
            task["scopes"] = sorted(chain(preprocess.scopes, task["scopes"]))
            add_capabilities_for_scopes(task)
            if env is not None:
                assert set(task["payload"]["env"]).isdisjoint(set(env))
                task["payload"]["env"].update(env)

            preprocess_task_id = slugId()
            yield preprocess_task_id, task

        for i in range(1, self.tasks + 1):
            task = yaml.safe_load(
                FUZZING_TASK.substitute(
                    task_group=parent_task_id,
                    description=DESCRIPTION.replace("\n", "\\n"),
                    created=stringDate(now),
                    deadline=stringDate(now + timedelta(seconds=self.max_run_time)),
                    expires=stringDate(fromNow("1 week", now)),
                    task_id=self.task_id,
                    owner_email=OWNER_EMAIL,
                    provisioner=PROVISIONER_ID,
                    scheduler=SCHEDULER_ID,
                    pool_id=self.pool_id,
                    secret=DECISION_TASK_SECRET,
                    max_run_time=self.max_run_time,
                    name=f"Fuzzing task {self.task_id} - {i}/{self.tasks}",
                )
            )
            task["payload"]["artifacts"].update(
                self.artifact_map(stringDate(fromNow("1 week", now)))
            )
            # `container` can be either a string or a dict, so can't template it
            task["payload"]["image"] = self.container
            if preprocess_task_id is not None:
                task["dependencies"].append(preprocess_task_id)
            task["scopes"] = sorted(chain(self.scopes, task["scopes"]))
            add_capabilities_for_scopes(task)
            if env is not None:
                assert set(task["payload"]["env"]).isdisjoint(set(env))
                task["payload"]["env"].update(env)

            yield slugId(), task


class PoolConfigMap(CommonPoolConfigMap):
    RESULT_TYPE = PoolConfiguration

    @property
    def task_id(self):
        return f"{self.platform}-{self.pool_id}"

    def build_resources(self, providers, machine_types, env=None):
        """Build the full tc-admin resources to compare and build the pool"""

        # Select a cloud provider according to configuration
        assert self.cloud in providers, f"Cloud Provider {self.cloud} not available"
        provider = providers[self.cloud]

        pools = list(self.iterpools())
        all_scopes = tuple(set(chain.from_iterable(pool.scopes for pool in pools)))

        # Build the pool configuration for selected machines
        machines = self.get_machine_list(machine_types)
        config = {
            "minCapacity": 0,
            "maxCapacity": max(sum(pool.tasks for pool in pools) * 2, 3),
            "launchConfigs": provider.build_launch_configs(
                self.imageset, machines, self.disk_size
            ),
            "lifecycle": {
                # give workers 15 minutes to register before assuming they're broken
                "registrationTimeout": parse_time("15m"),
                "reregistrationTimeout": parse_time("4d"),
            },
        }

        # Build the decision task payload that will trigger the new fuzzing tasks
        decision_task = yaml.safe_load(
            DECISION_TASK.substitute(
                description=DESCRIPTION.replace("\n", "\\n"),
                owner_email=OWNER_EMAIL,
                task_id=self.task_id,
                provisioner=PROVISIONER_ID,
                scheduler=SCHEDULER_ID,
                max_run_time=parse_time("1h"),
                secret=DECISION_TASK_SECRET,
            )
        )
        decision_task["scopes"] = sorted(chain(decision_task["scopes"], all_scopes))
        add_capabilities_for_scopes(decision_task)
        if env is not None:
            assert set(decision_task["payload"]["env"]).isdisjoint(set(env))
            decision_task["payload"]["env"].update(env)

        pool = WorkerPool(
            workerPoolId=f"{WORKER_POOL_PREFIX}/{self.task_id}",
            providerId=PROVIDER_IDS[self.cloud],
            description=DESCRIPTION.replace("\n", "\\n"),
            owner=OWNER_EMAIL,
            emailOnError=True,
            config=config,
        )

        hook = Hook(
            hookGroupId=HOOK_PREFIX,
            hookId=self.task_id,
            name=self.task_id,
            description=DESCRIPTION,
            owner=OWNER_EMAIL,
            emailOnError=True,
            schedule=list(self.cycle_crons()),
            task=decision_task,
            bindings=(),
            triggerSchema={},
        )

        role = Role(
            roleId=f"hook-id:{HOOK_PREFIX}/{self.task_id}",
            description=DESCRIPTION,
            scopes=decision_task["scopes"],
        )

        return [pool, hook, role]

    def build_tasks(self, parent_task_id, env=None):
        """Create fuzzing tasks and attach them to a decision task"""
        now = datetime.utcnow()

        for pool in self.iterpools():
            for i in range(1, pool.tasks + 1):
                task = yaml.safe_load(
                    FUZZING_TASK.substitute(
                        task_group=parent_task_id,
                        description=DESCRIPTION.replace("\n", "\\n"),
                        created=stringDate(now),
                        provisioner=PROVISIONER_ID,
                        scheduler=SCHEDULER_ID,
                        deadline=stringDate(now + timedelta(seconds=pool.max_run_time)),
                        expires=stringDate(fromNow("1 week", now)),
                        task_id=self.task_id,
                        name=(
                            f"Fuzzing task {pool.platform}-{pool.pool_id} - "
                            f"{i}/{pool.tasks}"
                        ),
                        owner_email=OWNER_EMAIL,
                        pool_id=pool.pool_id,
                        secret=DECISION_TASK_SECRET,
                        max_run_time=pool.max_run_time,
                    )
                )
                task["payload"]["artifacts"].update(
                    pool.artifact_map(stringDate(fromNow("1 week", now)))
                )
                # `container` can be either a string or a dict, so can't template it
                task["payload"]["image"] = pool.container
                task["scopes"] = sorted(chain(pool.scopes, task["scopes"]))
                add_capabilities_for_scopes(task)
                if env is not None:
                    assert set(task["payload"]["env"]).isdisjoint(set(env))
                    task["payload"]["env"].update(env)

                yield slugId(), task


class PoolConfigLoader:
    @staticmethod
    def from_file(pool_yml):
        assert pool_yml.is_file()
        data = yaml.safe_load(pool_yml.read_text())
        for cls in (PoolConfiguration, PoolConfigMap):
            if set(cls.FIELD_TYPES) >= set(data) >= cls.REQUIRED_FIELDS:
                return cls(pool_yml.stem, data, base_dir=pool_yml.parent)
        LOG.error(
            f"{pool_yml} has keys {data.keys()} and expected all of either "
            f"{PoolConfiguration.REQUIRED_FIELDS} or {PoolConfigMap.REQUIRED_FIELDS} "
            f"to exist."
        )
        raise RuntimeError(f"{pool_yml} type could not be identified!")
